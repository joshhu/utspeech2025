## 什麼是 Vibe Coding

### 什麼是 Vibe
- 正面的英文，感覺，正面字
- 例如smell動詞是**聞**，但本身是負面字
- Vibe就是好的感覺

### 什麼是 Coding
- 人類文明建立在數位世界上
- 在沒有更進一步的 representation 出現前，人類和電子世界溝通的方式就是**寫程式**
- 人類社會建立在**有電**的事物上
- 任何需要電的東西都跑在程式上
- 程式是用程式語言寫出來的
- 電腦只看得懂高電位(1)和低電位(0)
- 人類會自然語言
- 怎麼樣讓電腦和人類溝通，如同你到非洲，用英文和當地使用剛果語的人溝通，使用一個中介語言
- 這就是程式語言(Programming Language)的作用
- 目前流行的程式語言大概有數十種

### Vibe Coding 字面意思
Vibe Coding 字面上就是**爽爽的寫程式**。
- 2025年2月8號大神 Andrej Karpathy 在 X 上提出
- 之後全球開始瘋狂使用
- 比物件導向還重要的範式轉換(Paradigm Shift)

### 寫程式的目的
- Vibe Coding 的目的不是寫程式
- 寫程式是為了解決某個問題或滿足某個需求，主要是做一個軟體專案
- **寫程式**這件事本身佔了太多時間，要打字，查資料，試錯
- 但寫程式只是完成整個專案的非常小的一部分。後面會說更多

### 自然語言 vs 程式語言
- 都有單字，但程式語言較少
- 就是所謂的關鍵字，COBOL 有300個，最少的是 Lisp 0個
- 都有文法，程式語言較不複雜
- 寫出文法正確語句才能清楚表達意思
- 複雜度不一樣
  - 哈利波特全集290萬字
  - 一個 Windows 11 系統可能超過數千萬**行**的程式碼，
- 容錯能力不同
  - 自然語言容錯率高，出錯還看得懂
  - 程式語言**一個 bit 都不能有錯**

### Coding 為什麼不爽？
- 程式語言和自然語言一樣不好學
- 以英文文法為主
- 一點錯都不能有
- 寫程式本身就是違反人性的事


## 為什麼 AI 會寫程式？

### 目前的 AI 是什麼(LLM)
- LLM 其實是一個壓縮檔
- 使用的資料某有數百 TB(主要是 Common Crawl)，全世界的文字
- 使用一種稱為**Transformer**的深度神經網路將數百 TB 的文字資料壓縮到數百 GB 的檔案中，一般稱為**模型**。
- 這個檔案中充滿了參數(如 y=ax+b 中的 a, b，但函式內容更複雜)，可能有數百億個
- 這個檔案將數百 TB 的文字轉換另一種座標的表示法，個新座標的表示法有多少維度不一定，這個新的座標稱為 Latent Space，每一個維度稱為 Latent Variable
- 這些龐大數量的參數就是保存這些壓縮後的資訊

### LLM 的預訓練
- 先將文字轉換成完全正交的向量(稱為 one-hot)，其它 one-hot 的編碼方式如 RGB 就是完全正交的
- 想辦法將這些正交的向量壓縮到隱空間的維度中，通常是降維，如 llama4 的5120維
- 想像一下在電競螢幕上玩3D遊戲，將3D的座標轉換成2D平台的座標，**標準的矩陣變換**
- 好的 GPU 是必須的，一片5090貴的時候要到20萬台幣，32GB
- 但是有數百 TB 的文字，要從數萬維的正交轉換成數千維的隱空間，計算量更大
- 需要數萬顆5090等級的 GPU，花上數週到數個月的運算才能轉換完成
- 成本從數百萬至數億美金，非常耗電
- 在轉換過這些資料之後，視為**學習**後將知識消化成自己的智慧

### LLM 看過哪些資料？
- 全世界所有語言的文字，其實所有 LLM 看過的文字都差不多
- 全世界所有程式語言，包括 GitHub、Stack Overflow 等
- 大約是英文30%、簡中16%、程式語言10%，剩下是其它語言

### LLM 如何產生程式碼？
- 目前大部分的 LLM 是一個迴歸模型 Auto Regressive，就是拿自己生出來的值做迴歸
- 對模型輸入一個值，例如「台灣大」，它會根據這個值輸入下一個 token。生成的方式是從1萬2千個中文字中(甚至是更多的英文 token)中，找出機率最高的幾個，並且用丟骰子的方式從這幾個挑出來，例如「學」、「哥大」、「車隊」
- 如果挑出來的是「學」，它會把「台灣大學」再輸入至模型中，然後再找出機率最高的幾個來挑，例如「生」、「校」、「圖」。
- 一直重複這個過程，直到輸出「<結束符號>」為止。
- 依照這個辦法，可能輸出到幾萬個字之後才會停止
- 程式碼就是這樣寫出來的
- 厲害的是寫了幾十萬個 token 的程式，都還不會出錯能執行

## 你的第一次 Vibe Coding 經驗

### 誰沒用過 ChatGPT 寫程式？
- 大家用過 Vibe Coding 嗎？
- 每個人第一次的 Vibe Coding 經驗就是在 ChatGPT

### 用 ChatGPT 寫程式會遇到的事：
- 你還是得準備一個本機執行環境吧？(VS Code/PyCharm/記事本/Colab)
- 如果是 Html/CSS/JS，可能還可以在 ChatGPT 的 Canvas 上執行
- 出現錯誤(Module numpy not found)，然後 pip，然後 pip 錯誤、然後說沒裝 Python，跑去官網下載 Python，結果下載錯誤版本又不能跑...
- 要寫第二個程式，結果發現會和原來的程式套件架
- 光安裝環境就一大堆問題，更不要說寫程式了。
- 之前用的 venv, conda 要怎麼弄？ChatGPT 不會幫你弄
- 第一個問題，你的環境還是要自己搞定

### 如果很多檔案呢？
- 程式不只有程式檔，網頁有 html, css, js, 
- Python 有.py，有 requirements.txt、有 config.toml，
- 環境設定有.env
- 如果是要部署到容器雲有 Dockerfile
- 更不要說 README.md 等說明檔案
- 大部分專案有很複雜的結構
- 第二個問題，多檔無法上傳到 ChatGPT

### 程式出錯還好，不出錯但結果不是你要的
- compile error、runtime error 都是小事，貼給 ChatGPT 就行
- 如果是 logic error，程式能執行，不是你要的結果，**但沒有錯誤訊息**
- 解決這個問題，我們需要使用**測試**
- TDD、先寫測試再寫程式，提高測試的覆蓋率
- 第三個問題，ChatGPT 不會幫你寫測試

### 增加功能怎麼辦？
- 要加入 recaptcha、增加新功能
- 整個程式的效能不好要檢查
- 第四個問題，程式要在執行一段時間後重構(Refactor)

### 專案開發完成要放在哪裏提供服務？
- 不可能在自己電腦上
- 一般是放在 IaaS(GCE、EC2、Azure)
- 或是放在 PaaS(Render、Vercel、Github Action)
- 資料庫放在 DaaS(Supabase、Firebase)
- 如果是 Container，要放在 CaaS、如 k8s 和上面廠商的容器雲
- 第五個問題，ChatGPT 不會幫你部署，頂多寫一個教學

### 如何縮放(Scale)整個專案
- 平常網站沒人，遇到1111時整個當機
- 購票網站平常沒人，搶票時誰都搶不到票
- 疫苖網站
- 第六個問題，網站需要 Scaling

## 為什麼 Vibe Coding 在2025年開始流行

### 所有的 LLM 都是封閉的肥宅
- LLM 的預訓練使用大量的文字，處理文字需要花很多時間(所以 Meta 要收購 Scale AI)
- 使用最快速，最大量的 GPU 叢集，預訓練一個 LLM 也需要至少數個月的時間
- 在訓練開始時，收集了全世界的資料，但訓練結束時，中間經歷過的所有事它都不知道
- 如果問它這段時間發生的事，好的 LLM 會說不知道，但大部分的 LLM 會儘量生成出符合這個模型機率分佈的 token，這就是 LLM 的幻覺現象，**一本正經說幹話**
- 一般的 LLM 會標明自己的**知識截止日**(Knowledge Cutoff Date)

### 讓 LLM 多加思考的 Reasoning 功能
- LLM 不會計算，為何他能做出一些簡單的算數？
- 因為他在全世界的資料中，早就看過這些算數的結果了。
- 如果複雜一點的算數他就不會做了(一定做錯)
- 更複雜的邏輯問題，例如**我老婆是你媽媽的妹妹，我兒子是你爸爸的誰？**
- 在訓練 LLM 時，科學家發現只要讓 LLM 多加思考，它自己會將大問題拆解成小問題
- 這個過程叫思維鏈(Chain of thought)
- 這和人類解決複雜問題的方法完全一樣
- 模型會自己輸出過程，並且再吃回自己產生的過程不斷檢查、重複產生、比較、很像人類的思考過程
- 當最後不斷重複，直到輸出**<思考結束>**之後，才會輸出最後的答案給使用者
- 具備思考能力的模型成為主流

### 讓 LLM 飛起來的工具使用(Tool Use)
- 前面讓 LLM 算較難算術題目時，為何需要 LLM 來做呢？為何不用計算機算一下就好？
- LLM 只會文字接龍，它怎麼會用計算機
- 有一個方法，就是當需要使用計算機時，LLM 就自己生成「舉手！我要用計算機」
- LLM 在思考時，應該會導出生成這句話的文字
- 這樣我們就要幫 LLM 準備很多不同的工具，如「計算機」、「查天氣」、「寄電子郵件」......太麻煩了。
- 上面的工作，其實用程式來做就可以了。既然 LLM 能生出文字，自然能生出他需要結果的程式碼
- 例如上面的算術問題，他只要用 Python 寫一段程式來計算結果就行
- 但他寫程式在哪執行呢？(後面會提到)
- 每次都寫程式太麻煩，而且有重複的問題，浪費 token。不如我們將程式先寫好成函式，然後 LLM 只要產生一個 JSON 字串，包括需要執行的函式和參數即可
- 當 LLM 被問到特定問題時，它會思考出需要輸出這個 JSON 字串以及對應的工具，然後就生成出來，但不是給使用者，要給誰呢？

### Agent 就是 orchistrate LLM 的環境
- 提供一個環境，接管整個 LLM 工具輸出 JSON
- 當接收到 JSON 工具呼叫時，就找出對應的工具(函式)，然後在特定的環境中執行
- 執行之後會產生結果，再將結果給 LLM
- LLM 收到 Agent 傳回來的結果會再思考，決定是否要回傳給使用者
- 再進行 Auto regressive 的階段後，發現了**<結束符號>**，就將產生的所有結果傳回給使用者
- 如此一來，LLM 就具備和環境互動的能力
- 雖然有知識截止日期，但因為 LLM 有**上網搜尋**的能力，因此可以查到最新的資料，就不再是封閉的肥宅了。
- 大部分的 Agent 除了提供工具執行的能力，甚至有 bash、powershell 等本機執行能力，**這就不就可以用 uv 或 npm 安裝環境了嗎！**
- 但市面上這麼多服務，每個服務都有自己的 API，那怎麼辦？LLM 的工具有限。
- 另外操作瀏覽器也是一個必須，但 Agent 怎麼操作瀏覽器？

### MCP 一統所有服務
- MCP(Model Context Protocol)幫 LLM 裝上新的手腳
- 只要對應的廠商提供 MCP，所有支援工具的 LLM 就可以用上這個 MCP 的工具
- 甚至可以操作電腦

### System Prompt 的重要性
- LLM 是一個無狀態的數學模型，每一次輸入都會重新產生新的資料
- 但既然你要叫 ChatGPT，至少要會 Chat 吧！要記得所有對話
- 方法就是把第一次提問的問題+第一次回答的答案，加上第二次提問的問題一起丟入 LLM 中，讓他回答，依此類推
- 聊天的每一輪都會越來越巨大
- 儲存這些所有聊天記錄的地方，稱為 Context Window
- 較新的 LLM 甚至擁有200萬個 token 的 Context Window 大小
- 有些固定的內容，每次都要輸入，會自動加在使用者問題的前面
- 例如**今天的日期**、**你所在的位置**等。甚至是語氣、規定等。
- 這個內容稱為 System Prompt
- 事實上 system prompt 只是 ChatML 規定的 chat template 中的一個角色(role)。還有使用者角色的問題(user role)以及 LLM 回答的 assistant role。
- 所有 role 的內容都會被疊加成某種形式再丟回 LLM 完成多輪對話

## Context Engineering 是什麼？

### 機器學習的範式演變
- 機器學習最早是想辦法取得資料對於整個模型的代表性，要想辦法找出資料的特徵，需要人工自己找，如**詞頻**、**出現位置**等。這種手動尋找特徵的機器學習範式稱為**Feature Engineering**
- 後來發明了深度神經網路，但我們不斷在試探新的網路架構，如從 CNN 換成 RNN、或是換成**Transformer**。這種尋找架構的範式稱為**Architecture Engineering**
- 後來我們發現同一架構，可以使用不同的目標來獲得結果，這種探索目標的方式稱為**Objective Engineering**
- 之後發現整個網路完全不用更動參數，只要使用不同的提示詞就可以輸出不同的結果，而探索提示詞的方式稱為**Prompt Engineering**
- 到了2025年，LLM 的能力完全不同，加上 Agent 的能力，我們需要控制探索 Context Window 的範式，稱為**Context Engineering**

### Context Engineering 的特色
- 把 LLM 看成一個作業系統，Context Window 就是他的記憶體
- 每次對話要在記憶體中放入什麼資料，需要很精密的計劃
- 不追求神奇 prompt，建立流程，把**正確的資訊與工具、以正確格式、在正確時間**塞進模型的上下文
- token 不只是文字，圖、影、音都可以被轉換為 token 被入上下文
- RAG 切塊的技巧
- 位置的重要性，論文提到**Lost in the middle**問題
- **壓縮與優先級**的重要性
- 結構化輸入/輸出與格式約定：把檢索結果、工具回傳與系統規則整理成易於解析的段落、表格、JSON schema 或明確段落標頭
- 可靠性與安全護欄，在 Context Window 中加入明確邊界條款（不得編造、可回答「不知道」）
- 遙測與離線評估（eval loop）：以任務分數、引用正確率、事實比對等指標迭代調整「檢索→壓縮→注入→工具」的管線（而非只調 prompt ）
- 與產品需求緊耦合：成功與否取決於對業務輸出（格式、受眾、決策環節）的清晰定義

## 現在才來談Vibe Coding的重要規則
### 定義與邊界
- Vibe Coding 定義：以自然語言/對話驅動的專案開發流程
- 人類以需求與回饋引導 LLM 產生、修補與重構程式，而不以逐行手寫為主（Karpathy, 2025）。
- 與「AI 協助寫碼」區別：若工程師完整審閱並理解每行程式再合併，屬於「AI 輔助」而非 vibe coding（Willison, 2025）。
- 風險假設：預設 LLM 產出程式碼存在安全、相依、可維護性風險；所有策略均以可驗證證據（測試、靜態分析、SBOM、審計紀錄）作為收斂依據。

### 需求→設計→計畫先於產出程式碼（Plan-before-Code）
- 以 PRD/ADR（Architecture Decision Record）寫清：目標、約束、資料流、外部介面、成功準則與風險。
- 強制模型先輸出計畫與模組切分（steps／WBS／API 合約與資料結構），經人審再允許進入產出程式碼階段。
- 產出與計畫自動同步（在 repo 內生成 `/docs`、`/design`、`/contracts` 目錄並版本控管）。

### 上下文工程（Context Engineering）作為核心工序
- 上下文組態層：將規格、合約、範例、風險列表、測試骨架作為顯式上下文輸入，避免隱含假設。
- 檔案/知識檢索：以 RAG/向量索引選入最小充分子集；避免無關檔案污染生成。
- 上下文壓縮：摘要、關鍵函式抽取、介面簽章提取與程式切塊（chunking with boundaries）以適配 context window。

### 以「可執行事實」收斂（Tests-as-Truth）
- 在生成前先產生可執行規格（測試樣例、合約測試、property-based tests、安全測試腳本）。
- 對每次 AI 變更執行：單元/整合/端到端/安全掃描（SAST/DAST）、授權/授信測試（authz/authn）。
- 將測試通過作為合併門檻；把測試失敗訊號回饋給模型迭代（fail → reason → patch）。

### 介面先行與邊界明確（Contracts-first）
- 所有跨模組用 IDL/Schema（OpenAPI/JSON Schema/Protobuf）先行固化；模型按合約生成 stub/impl。
- 以適配器/Facade 隔離第三方 API 與不穩定依賴，降低重生程式（regeneration）對核心域的衝擊。

### 生成管線與環境可重現（Reproducibility）
- 使用鎖定檔與可重現建置（`poetry.lock`/`package-lock.json`/`uv.lock`、容器鏡像 digest）。
- 追蹤 SBOM（CycloneDX/SPDX），比對供應鏈風險；CI 強制 License/漏洞守門。
- 對每次 LLM 呼叫保存：提示、模型版本、上下文來源、產出雜湊，供審計與回溯。

### 觀測性與審計（Observability & Auditability）
- 針對生成與執行路徑埋設遙測：日誌、追蹤、度量；建立變更日誌（changelog）自動生成。
- 對提示/回應加上紅隊標註（prompt injection / data exfiltration）分類，持續微調防護規則。

### 人在迴圈內的風險治理（Human-in-the-Loop）
- 人工必審：資料處理/隱私、金流、權限、加密、錯誤處理、可用性 SLA 等敏感區塊。
- 對非關鍵路徑允許快速接受（accept-and-iterate）；對關鍵路徑採雙人審/變更委員會。

### 安全預設最小權限（Least Privilege by Default）
- 將執行代理（agents/tools）以零信任/沙箱隔離；對外呼叫白名單化，API key 以密鑰管理服務託管。
- 自動產出程式碼強制過秘密檢測、危險函式黑名單、規則式 SAST（例如：禁止 `eval`、SQL 直串）。

### 模型/工具鏈分工與版本化
- 規劃/推理用高推理模型；批量重構/生成用效能型模型；明確切換點與輸入輸出契約。
- 對模型版本與提示模板版本化；依工作負載建立評測基準（coverage、latency、bug intro rate）。

### MVP 原則與漸進交付（MVP-driven Development）
- 以最小可行產品思維驅動生成：每次迭代產出可執行、可驗證的最小功能集，避免過度工程化。
- 優先實現核心價值主張，延後次要功能；讓早期使用者盡快體驗與回饋。
- 建立假設驗證循環：功能假設 → 最小實現 → 使用者測試 → 數據收集 → 假設修正。

### 迭代節奏（Tight Loop）
規劃（Plan）→ 生成（Generate）→ 執行（Run）→ 觀測（Observe）→ 封裝回饋（Constrain/Context Update）→ 重複
- 每輪輸出 Diff + 通過測試證據；嚴禁「一口氣大改」超過可審查尺度。

### 專案規模化（Scale-out）
- 以功能旗標/實驗控制風險釋出；灰度上線與回滾策略標準化。
- 用任務圖（task graph）/DAG 驅動多代理協作，節點輸入輸出完全可追蹤。

### 既有工程制度整合
- 保留設計審查、程式碼風格、分支策略、釋出準則等既有制度。
- Vibe Coding 與傳統工具（IDE、專案管理、文件協作）並行使用；以治理與品質為主線。

---

#### 參考與延伸閱讀（部分）
- Karpathy, A. 「There's a new kind of coding I call 'vibe coding' ...」(Tweet, 2025-02)  
- Simon Willison, *Not all AI-assisted programming is vibe coding* (2025-03)  
- Google Cloud, *What is Vibe Coding?* (2025-??)  
- IBM Think, *Vibe coding topic page* (2025-04-08)  
- Financial Times, *'Vibe coding' is the new DIY* (2025-06)  
- Business Insider, *YC Demo Day: vibe coding startups* (2025-09)

